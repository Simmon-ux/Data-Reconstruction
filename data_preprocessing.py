# -*- coding: utf-8 -*-
"""Data_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17NKJUiiD7sgWuFqJ0PU3iHDvFsjq37wA
"""

!pip install netCDF4
!apt-get update
!apt-get install -y cdo

from google.colab import drive
drive.mount('/content/drive')

import os, re, glob, time, gc
import numpy as np
import pandas as pd
import xarray as xr
from pyproj import Transformer
from netCDF4 import Dataset
import dask
from dask.diagnostics import ProgressBar

# --- Part 2: Data Processing and Aggregation (clean & fast, single-source GHCN_D) ---
import re
import numpy as np
import pandas as pd
import xarray as xr
from pyproj import Transformer

# Load observations
df = pd.read_csv('/content/drive/MyDrive/GHCN_D.csv')

# EASE-Grid 2.0 parameters (South hemisphere, EPSG:6932)
grid_size_m  = 100_000
Grid_rows    = 180
Grid_columns = 180
x_min = -9_000_000           # left edge
y_max =  9_000_000           # TOP edge  ← changed name + meaning

# Project lon/lat -> EASE (axis order safe)
transformer = Transformer.from_crs("EPSG:4326", "EPSG:6932", always_xy=True)
df['x_EASE'], df['y_EASE'] = transformer.transform(df['LONGITUDE'].values, df['LATITUDE'].values)

# Map to grid indices (left->right for cols, top->bottom for rows)
df['grid_col'] = np.floor((df['x_EASE'] - x_min) / grid_size_m).astype(int)
df['grid_row'] = np.floor((y_max - df['y_EASE']) / grid_size_m).astype(int)   # ← flipped sign

# Keep strictly in-bounds (drop exact outer-edge = 9,000,000 cases -> index 180)
df = df[(df['grid_col'] >= 0) & (df['grid_col'] < Grid_columns) &
        (df['grid_row'] >= 0) & (df['grid_row'] < Grid_rows)]

# SINGLE-SOURCE LAND-ONLY: no land/ocean weighting needed
df['area_weight'] = 1.0

# Robust date column detection (allow 1–2 digit day/month)
date_cols = [c for c in df.columns if re.fullmatch(r'\d{1,2}_\d{1,2}_\d{4}', c)]
if not date_cols:
    raise ValueError("No date-like columns found (expect dd_mm_yyyy or d_m_yyyy).")

# Melt to long format
df_melted = df.melt(
    id_vars=['grid_row', 'grid_col', 'area_weight'],
    value_vars=date_cols,
    var_name='date',
    value_name='temperature'
)

# Parse & clean BEFORE grouping
df_melted['date'] = pd.to_datetime(df_melted['date'], format='%d_%m_%Y', errors='coerce')
df_melted['temperature'] = pd.to_numeric(df_melted['temperature'], errors='coerce')
df_melted = df_melted.dropna(subset=['date','temperature'])

# Weighted mean per (cell, date)
gridded = (df_melted
           .groupby(['grid_row','grid_col','date'])
           .apply(lambda x: np.average(x['temperature'], weights=x['area_weight']),
                  include_groups=False)
           .to_frame('avg_SAT')
           .reset_index()
           .sort_values(['date','grid_row','grid_col']))

# Vectorized pivot to xarray
ds = (gridded
      .set_index(['grid_row','grid_col','date'])['avg_SAT']
      .to_xarray()
      .to_dataset(name='avg_SAT'))

# Save gridded obs (pre-normalization)
ds.to_netcdf('/content/drive/MyDrive/gridded_temperature_data.nc')

# =========================
# Regrid & Normalize
# =========================
import warnings
import os, glob
import numpy as np
import xarray as xr
from pyproj import Transformer

warnings.filterwarnings(
    "ignore",
    message=r"invalid value encountered in divide",
    module=r"dask\.array\.numpy_compat",
)

# -------------------- Paths & config --------------------
FILE_PATTERN = "/content/drive/MyDrive/nc_files/air.2m.gauss.*.nc"   # NCEP/NCAR (var='air' in Kelvin)
OBS_FILE     = "/content/drive/MyDrive/gridded_temperature_data.nc"

OUT_REGRID   = "/content/drive/MyDrive/regridded_reanalysis.nc"
OUT_OBS_NZ   = "/content/drive/MyDrive/normalized_observation_data.nc"
OUT_REAN_NZ  = "/content/drive/MyDrive/normalized_reanalysis_data.nc"
OUT_MASK     = "/content/drive/MyDrive/obs_mask_ease_100km_180x180.nc"

# EASE SH 100 km, 180x180
GRID_ROWS, GRID_COLS = 180, 180
GRID_M = 100_000
X_MIN  = -9_000_000
Y_MAX  =  9_000_000

# Period for climatology
TSTART = "1979-01-01"
TEND   = "2023-12-31"

# I/O chunking
TIME_CHUNK  = 120
SPACE_CHUNK = 90
FAST_WRITE  = True
MIN_COUNT   = 2
EPS_SIGMA   = 1e-6

# -------------------- helpers --------------------
def ensure_lat_ascending(ds, lat_name="lat"):
    if lat_name in ds.coords and ds[lat_name].ndim == 1:
        lat = ds[lat_name]
        if (lat[1] - lat[0]) < 0:
            ds = ds.reindex({lat_name: lat[::-1]})
    return ds

def enc_for(da, dtype="float32", fast=True):
    chunks = []
    for d in da.dims:
        if d in ("time", "date"):
            chunks.append(min(TIME_CHUNK, da.sizes[d]))
        elif d in ("grid_row","grid_col"):
            chunks.append(min(SPACE_CHUNK, da.sizes[d]))
        else:
            chunks.append(da.sizes[d])
    enc = {"dtype": dtype, "chunksizes": tuple(chunks)}
    if not fast:
        enc.update({"zlib": True, "complevel": 4})
    return enc

# --- Blockwise, warning-free normalization ---
def _safe_norm_block(x, mu, sig):
    out = np.full_like(x, np.nan, dtype=np.float32)
    valid = np.isfinite(x) & np.isfinite(mu) & np.isfinite(sig) & (sig > 0)
    denom = np.maximum(sig.astype(np.float32), EPS_SIGMA)  # never <= 0
    np.divide((x - mu).astype(np.float32), denom, out=out, where=valid)
    return out, valid.astype(np.int8)

def safe_norm(x, mu_m, sig_m):
    z, valid = xr.apply_ufunc(
        _safe_norm_block,
        x, mu_m, sig_m,
        input_core_dims=[[], [], []],
        output_core_dims=[[], []],
        dask="parallelized",
        output_dtypes=[np.float32, np.int8],
        vectorize=True,
    )
    return z, valid

# -------------------- 1) Load reanalysis & regrid to EASE --------------------
files = sorted(glob.glob(FILE_PATTERN))
if not files:
    raise FileNotFoundError(f"No files match {FILE_PATTERN}")

dsr = xr.open_mfdataset(files, combine="by_coords", chunks={"time": TIME_CHUNK})
dsr = dsr.sel(time=slice(TSTART, TEND))

# coord cleanups
ren = {}
if "latitude" in dsr.coords and "lat" not in dsr.coords:
    ren["latitude"]  = "lat"
if "longitude" in dsr.coords and "lon" not in dsr.coords:
    ren["longitude"] = "lon"
if ren:
    dsr = dsr.rename(ren)
if float(dsr["lon"].max()) > 180:
    dsr = dsr.assign_coords(lon=((dsr["lon"] + 180) % 360) - 180).sortby("lon")
dsr = ensure_lat_ascending(dsr, "lat")

# EASE grid centers -> lon/lat
to_ll = Transformer.from_crs("EPSG:6932", "EPSG:4326", always_xy=True)
x = X_MIN + (np.arange(GRID_COLS) + 0.5) * GRID_M
y = Y_MAX - (np.arange(GRID_ROWS) + 0.5) * GRID_M
xx, yy = np.meshgrid(x, y)
lon_t, lat_t = to_ll.transform(xx, yy)

target = xr.Dataset(coords={
    "grid_row": np.arange(GRID_ROWS, dtype="int64"),
    "grid_col": np.arange(GRID_COLS, dtype="int64"),
    "lat": (("grid_row","grid_col"), lat_t.astype("float64")),
    "lon": (("grid_row","grid_col"), lon_t.astype("float64")),
})

print("Starting bilinear interpolation (reanalysis → EASE grid)...")
air_ease = (
    dsr["air"]
    .chunk({"time": TIME_CHUNK, "lat": dsr.sizes["lat"], "lon": dsr.sizes["lon"]})
    .interp(lat=target["lat"], lon=target["lon"], method="linear", kwargs={"fill_value": np.nan})
    .astype("float32")
    .transpose("time","grid_row","grid_col")
    .chunk({"time": TIME_CHUNK, "grid_row": SPACE_CHUNK, "grid_col": SPACE_CHUNK})
)

xr.Dataset({"air": air_ease}).to_netcdf(
    OUT_REGRID,
    encoding={"air": {"dtype":"float32", "chunksizes":(TIME_CHUNK, SPACE_CHUNK, SPACE_CHUNK)}}
)
print(f"Regridding complete → {OUT_REGRID}")

# -------------------- 2) Load obs & align --------------------
dso   = xr.open_dataset(OBS_FILE).chunk({"grid_row": SPACE_CHUNK, "grid_col": SPACE_CHUNK})
ds_rg = xr.open_dataset(OUT_REGRID).chunk({"time": TIME_CHUNK, "grid_row": SPACE_CHUNK, "grid_col": SPACE_CHUNK})

obs_t = "time" if "time" in dso.dims else ("date" if "date" in dso.dims else None)
if obs_t is None:
    raise ValueError("Observations need a 'time' or 'date' dimension.")

obs  = dso["avg_SAT"]
rean = (ds_rg["air"] - 273.15).astype("float32")

# align grid & time
ds_rg = ds_rg.reindex(grid_row=obs["grid_row"], grid_col=obs["grid_col"])
common = np.intersect1d(dso[obs_t].values, ds_rg["time"].values)
if common.size == 0:
    raise ValueError("No overlapping timestamps after slicing 1979–2023.")
obs  = obs.sel({obs_t: common})
rean = rean.sel(time=common)

base = rean.sel(time=slice(TSTART, TEND))
mu = base.groupby("time.month").mean("time", skipna=True).astype("float32")   # (month,row,col)
mu_bt = mu.sel(month=base["time"].dt.month)                                   # (time,row,col)
sumsq = ((base - mu_bt)**2).groupby("time.month").sum("time", skipna=True).astype("float32")
cnt   = base.groupby("time.month").count("time")                               # integer counts

# population variance with safe denominator (never 0 during divide)
den   = xr.where(cnt > 0, cnt, 1)
var0  = (sumsq / den).astype("float32")

# σ: require enough samples and positive variance
sigma = xr.where((cnt >= MIN_COUNT) & np.isfinite(var0) & (var0 > 0), np.sqrt(var0), np.nan).astype("float32")

# -------------------- 4) Normalize safely  --------------------
# Reanalysis
m_rean = mu.sel(month=rean["time"].dt.month)
s_rean = sigma.sel(month=rean["time"].dt.month)
rean_z, rean_valid = safe_norm(rean, m_rean, s_rean)

# Observations
obs_times  = dso[obs_t].sel({obs_t: common})
obs_months = xr.DataArray(obs_times.dt.month, dims=[obs_t])   # 1..12
m_obs = mu.isel(month=(obs_months - 1))
s_obs = sigma.isel(month=(obs_months - 1))
m_obs = m_obs.reindex(grid_row=obs["grid_row"], grid_col=obs["grid_col"])
s_obs = s_obs.reindex(grid_row=obs["grid_row"], grid_col=obs["grid_col"])
obs_z, obs_valid = safe_norm(obs, m_obs, s_obs)

try:
    rn = float((~np.isfinite(rean_z)).mean().compute()) * 100.0
    on = float((~np.isfinite(obs_z)).mean().compute()) * 100.0
    print(f"NaN fraction before fill — rean_norm: {rn:.3f}% | obs_norm: {on:.3f}%")
except Exception:
    pass

# Fill invalid with 0.0; keep masks for training
rean_n = rean_z.fillna(0.0).astype("float32").chunk({"time": TIME_CHUNK, "grid_row": SPACE_CHUNK, "grid_col": SPACE_CHUNK})
obs_n  = obs_z.fillna(0.0).astype("float32").chunk({obs_t: TIME_CHUNK, "grid_row": SPACE_CHUNK, "grid_col": SPACE_CHUNK})

rean_n.name = "normalized_air"
obs_n  = obs_n.rename("normalized_SAT")
for da in (rean_n, obs_n):
    da.attrs.update({
        "units": "z",
        "long_name": "z-score normalized using reanalysis monthly μ,σ (1979–2023)",
        "note": "Invalid points (σ<=0, NaN σ, or NaN input) were masked and filled with 0.0 for training. See mask file.",
        "fill_value_used": 0.0,
    })

# Observation presence mask (1 where obs exists originally)
obs_mask = xr.where(np.isfinite(obs), 1, 0).astype("int8").rename("obs_mask") \
             .chunk({obs_t: TIME_CHUNK, "grid_row": SPACE_CHUNK, "grid_col": SPACE_CHUNK})
obs_mask.attrs.update({
    "long_name":"observation presence mask",
    "flag_values":[0,1],
    "flag_meanings":"missing present"
})

# Final finite check
assert bool(np.isfinite(rean_n.data).all().compute())
assert bool(np.isfinite(obs_n.data).all().compute())

# -------------------- Save outputs --------------------
for p in (OUT_OBS_NZ, OUT_REAN_NZ, OUT_MASK):
    os.makedirs(os.path.dirname(p), exist_ok=True)

xr.Dataset({"normalized_SAT":  obs_n}).to_netcdf(
    OUT_OBS_NZ,  encoding={"normalized_SAT":  enc_for(obs_n,  "float32", FAST_WRITE)}
)
xr.Dataset({"normalized_air": rean_n}).to_netcdf(
    OUT_REAN_NZ, encoding={"normalized_air": enc_for(rean_n, "float32", FAST_WRITE)}
)
xr.Dataset({"obs_mask":       obs_mask}).to_netcdf(
    OUT_MASK,   encoding={"obs_mask":       enc_for(obs_mask, "int8", FAST_WRITE)}
)

print("Done.")
print(f"  - Normalized Observation data: {OUT_OBS_NZ}")
print(f"  - Normalized Reanalysis data:  {OUT_REAN_NZ}")
print(f"  - Observation mask:            {OUT_MASK}")

